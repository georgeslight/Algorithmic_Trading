{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T15:39:33.441440Z",
     "start_time": "2024-11-19T15:39:20.691649Z"
    }
   },
   "source": [
    "import importlib\n",
    "import os\n",
    "from datetime import datetime, date\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from data_collection import DataCollection\n",
    "\n",
    "import multi_input_lsm\n",
    "importlib.reload(multi_input_lsm)\n",
    "from multi_input_lsm import MultiInputLSTM as lstm\n",
    "\n",
    "import data_plotter\n",
    "importlib.reload(data_plotter)\n",
    "from data_plotter import DataPlotter\n",
    "\n",
    "from algorithmic_trading.experiment_4.src import preprocessing\n",
    "\n",
    "importlib.reload(preprocessing)\n",
    "from algorithmic_trading.experiment_4.src.preprocessing import Preprocessing\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### DATA PREPARATION #####",
   "id": "c28aea76e9a13d1f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:40:40.522637Z",
     "start_time": "2024-11-19T15:40:31.986990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parameters\n",
    "predicted_days = 1\n",
    "tickers = [\"GOOGL\"]\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = datetime.today().date()\n",
    "sequence_length = 100 # todo 30, 50, 100, 365\n",
    "batch_size = 32 # todo 16, 32, 64\n",
    "data_path = \"../data\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Data collection\n",
    "data_collector = DataCollection(tickers, start_date, end_date, folder_path=data_path)\n",
    "data_collector.fetch_and_save_all()\n",
    "\n",
    "# Preprocessing\n",
    "pp = Preprocessing(folder_path=data_path, split_ratio=0.8, sequence_length=sequence_length)\n",
    "x_open_train, x_high_train, x_low_train, x_close_train, y_train, y_train_dates,x_open_test, x_high_test, x_low_test, x_close_test, y_test, y_test_dates = pp.preprocess_pipeline()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x_open_train_tensor = torch.tensor(x_open_train, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_high_train_tensor = torch.tensor(x_high_train, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_low_train_tensor = torch.tensor(x_low_train, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_close_train_tensor = torch.tensor(x_close_train, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "# x_volume_train_tensor = torch.tensor(x_volume_train, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "\n",
    "x_open_test_tensor = torch.tensor(x_open_test, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_high_test_tensor = torch.tensor(x_high_test, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_low_test_tensor = torch.tensor(x_low_test, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "x_close_test_tensor = torch.tensor(x_close_test, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "# x_volume_test_tensor = torch.tensor(x_volume_test, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(\n",
    "    x_open_train_tensor, x_high_train_tensor, x_low_train_tensor, x_close_train_tensor, y_train_tensor\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    x_open_test_tensor, x_high_test_tensor, x_low_test_tensor, x_close_test_tensor, y_test_tensor\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "ee646c0f38f79db0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data\\GOOGL_data.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### MODEL SETUP #####",
   "id": "ce35a95b92c819c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:40:48.152121Z",
     "start_time": "2024-11-19T15:40:42.299172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = 4  # ['Open', 'High', 'Low', 'Close']\n",
    "hidden_size = 128  # Number of hidden units in LSTM # todo 32, 50, 64, or 128\n",
    "# output_size = 4  # Predicting 5 values for the next day\n",
    "# num_layers = 2  # LSTM layers # todo 1 to 3\n",
    "# dropout = 0.2  # Dropout rate for regularization # todo 0.1 to 0.5\n",
    "learning_rate = 0.0001 # todo 0.001, 0.0005, 0.0001\n",
    "\n",
    "# Instantiate the model\n",
    "model = lstm(input_sz=input_size, hidden_sz=hidden_size).to(device) # todo Bidirectional LSTM, Gated Recurrent Unit (GRU)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss() # todo Mean Absolute Error (MAE = L1Loss()), Mean Squared Error (MSE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # todo Adam, AdamW"
   ],
   "id": "53aca998758e4ed0",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### TRAINING THE MODEL #####",
   "id": "87ed9838f92cdfdb"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-19T15:40:50.228011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 200 # todo 50, 200\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for x_open, x_high, x_low, x_close, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_open, x_high, x_low, x_close)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.6f}\")"
   ],
   "id": "df028633c8abbf0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.066297\n",
      "Epoch 2/200, Loss: 0.004652\n",
      "Epoch 3/200, Loss: 0.000997\n",
      "Epoch 4/200, Loss: 0.000521\n",
      "Epoch 5/200, Loss: 0.000409\n",
      "Epoch 6/200, Loss: 0.000336\n",
      "Epoch 7/200, Loss: 0.000295\n",
      "Epoch 8/200, Loss: 0.000273\n",
      "Epoch 9/200, Loss: 0.000262\n",
      "Epoch 10/200, Loss: 0.000246\n",
      "Epoch 11/200, Loss: 0.000252\n",
      "Epoch 12/200, Loss: 0.000250\n",
      "Epoch 13/200, Loss: 0.000226\n",
      "Epoch 14/200, Loss: 0.000219\n",
      "Epoch 15/200, Loss: 0.000231\n",
      "Epoch 16/200, Loss: 0.000219\n",
      "Epoch 17/200, Loss: 0.000208\n",
      "Epoch 18/200, Loss: 0.000202\n",
      "Epoch 19/200, Loss: 0.000198\n",
      "Epoch 20/200, Loss: 0.000197\n",
      "Epoch 21/200, Loss: 0.000192\n",
      "Epoch 22/200, Loss: 0.000184\n",
      "Epoch 23/200, Loss: 0.000178\n",
      "Epoch 24/200, Loss: 0.000175\n",
      "Epoch 25/200, Loss: 0.000174\n",
      "Epoch 26/200, Loss: 0.000179\n",
      "Epoch 27/200, Loss: 0.000169\n",
      "Epoch 28/200, Loss: 0.000172\n",
      "Epoch 29/200, Loss: 0.000153\n",
      "Epoch 30/200, Loss: 0.000159\n",
      "Epoch 31/200, Loss: 0.000161\n",
      "Epoch 32/200, Loss: 0.000146\n",
      "Epoch 33/200, Loss: 0.000151\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### EVALUATING THE MODEL #####",
   "id": "835f428bdd4be302"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import data_plotter\n",
    "importlib.reload(data_plotter)\n",
    "from data_plotter import DataPlotter\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_open, x_high, x_low, x_close, y in test_loader:\n",
    "         # Forward pass\n",
    "        outputs = model(x_open, x_high, x_low, x_close)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs.squeeze(), y)\n",
    "        test_loss += loss.item()\n",
    "        # Sotre predictions and actuals\n",
    "        predictions.append(outputs.cpu())\n",
    "        actuals.append(y.cpu())\n",
    "\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "\n",
    "# Post-processing predictions and actuals for inverse scaling\n",
    "predictions = torch.cat(predictions).numpy()\n",
    "actuals = torch.cat(actuals).numpy()\n",
    "\n",
    "# Inverse transform OHLC using pp.scaler\n",
    "predictions_original = pp.scaler.inverse_transform(predictions)\n",
    "actuals_original = pp.scaler.inverse_transform(actuals)\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Date: {y_test_dates[i]}\")\n",
    "    prediction_str = \", \".join([f\"{x:.2f}\" for x in predictions_original[i]])\n",
    "    actual_str = \", \".join([f\"{x:.2f}\" for x in actuals_original[i]])\n",
    "    print(f\"Sample prediction (original scale): {prediction_str}\")\n",
    "    print(f\"Actual values (original scale): {actual_str}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Plotting results\n",
    "plotter = DataPlotter()\n",
    "df_results = plotter.create_results_dataframe(y_test_dates, actuals_original, predictions_original)\n",
    "plotter.plot_results(df_results)"
   ],
   "id": "416da027b942fb9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Predict ######",
   "id": "9cb90104a78e7fa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import predictor\n",
    "importlib.reload(predictor)\n",
    "from predictor import Predictor\n",
    "\n",
    "# Initialize the Predictor\n",
    "predictor = Predictor(\n",
    "    model=model,\n",
    "    scaler=pp.scaler,\n",
    "    # volume_scaler=pp.volume_scaler,\n",
    "    sequence_length=sequence_length,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Specify parameters for prediction\n",
    "start_date = date(2023, 1, 3)\n",
    "\n",
    "days_to_predict = 10\n",
    "\n",
    "# Perform the prediction\n",
    "dataset = pp.load_data()  # Load the dataset to access dates\n",
    "dataset['Date'] = pd.to_datetime(dataset['Date']).dt.date\n",
    "predicted_values, predicted_values_scaled = predictor.predict_future(dataset, start_date, days_to_predict)\n",
    "\n",
    "# Extract actual values for comparison\n",
    "start_index = dataset.index[dataset['Date'] == start_date][0] + 1\n",
    "actual_values = dataset.iloc[start_index:start_index + days_to_predict][['Open', 'High', 'Low', 'Close']].values\n",
    "actual_values_scaled = pp.scaler.transform(actual_values)\n",
    "\n",
    "# Evaluate predictions using multiple metrics\n",
    "mse = np.mean((actual_values_scaled - predicted_values_scaled) ** 2)  # Mean Squared Error\n",
    "mae = np.mean(np.abs(actual_values_scaled - predicted_values_scaled))  # Mean Absolute Error\n",
    "variance = np.var(actual_values_scaled - predicted_values_scaled)  # Variance of errors\n",
    "\n",
    "# Compare predicted values with actual values\n",
    "print(f\"\\nPredictions vs Actuals starting from {start_date.strftime('%Y-%m-%d')} for {days_to_predict} days:\")\n",
    "\n",
    "for i, (prediction, actual) in enumerate(zip(predicted_values, actual_values)):\n",
    "    prediction_date = dataset.iloc[start_index + 1 + i]['Date']\n",
    "    # Compute changes\n",
    "    predicted_change = ((prediction - actual_values[i]) / actual_values[i]) * 100\n",
    "    \n",
    "    print(f\"Date: {prediction_date}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(f\"Actual: {actual}\")\n",
    "    print(f\"Change (%): {predicted_change}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "# Print evaluation metrics\n",
    "print(f\"Test Metrics:\")\n",
    "print(f\"MSE: {mse:.6f}\")  # Mean Squared Error -> Smaller values indicate better model performance.\n",
    "print(f\"MAE: {mae:.6f}\")  # Mean Absolute Error -> Provides a more interpretable metric compared to MSE (does not square the errors).\n",
    "print(f\"Variance of errors: {variance:.6f}\")  # Variance -> Lower variance indicates that the model consistently makes predictions close to the actual values.\n",
    "\n",
    "# Plot\n",
    "plotter = DataPlotter()\n",
    "# Create a DataFrame for results\n",
    "df_results = plotter.create_results_dataframe(\n",
    "    dates=dataset[\"Date\"].iloc[start_index + 1:start_index + 1 + days_to_predict],  # Use the dates for the prediction range\n",
    "    actuals_original=actual_values,\n",
    "    predictions_original=predicted_values\n",
    ")\n",
    "# Plot results for the prediction range\n",
    "plotter.plot_results(df_results\n",
    ")"
   ],
   "id": "5b5b2e6326def1e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simulating the \"Without Predictions\" Strategy",
   "id": "c9a789d2bb6e939c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import random\n",
    "import simulator\n",
    "importlib.reload(simulator)\n",
    "from simulator import InvestmentSimulator\n",
    "\n",
    "random_index = random.randint(0, len(dataset) - 1)\n",
    "values = dataset.iloc[random_index:][['Open', 'High', 'Low', 'Close']].values\n",
    "\n",
    "simulator = InvestmentSimulator(\n",
    "    prices=values,\n",
    "    initial_capital=1000,\n",
    "    profit_threshold=0.10,\n",
    "    loss_threshold=-0.05,\n",
    ")\n",
    "\n",
    "print(f\"Simulated buying stock in day: {dataset['Date'][random_index]}\")\n",
    "result = simulator.simulate()\n",
    "print(result)"
   ],
   "id": "81854969fbd75f18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Simulating the \"With Predictions\" Strategy",
   "id": "e02c91cb8d1ab2a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:27:20.460454Z",
     "start_time": "2024-11-19T15:27:20.460454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import predictor\n",
    "importlib.reload(predictor)\n",
    "from predictor import Predictor\n",
    "\n",
    "# Initialize Predictor\n",
    "predictor = Predictor(\n",
    "    model=model,  # Your trained model\n",
    "    scaler=pp.scaler,\n",
    "    sequence_length=sequence_length,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Random day\n",
    "start_date = pd.to_datetime(dataset['Date'][random_index]).date()\n",
    "# Values up to day at random_index\n",
    "past_values = dataset.iloc[:random_index + 1][['Date', 'Open', 'High', 'Low', 'Close']]\n",
    "\n",
    "i = 0\n",
    "result = None\n",
    "\n",
    "while result is None or result.get('status') not in ['profit', 'loss']:\n",
    "    i += 1\n",
    "    predicted_values, predicted_values_scaled = predictor.predict_future(past_values, start_date, i)\n",
    "    \n",
    "    simulator = InvestmentSimulator(\n",
    "        prices=predicted_values,\n",
    "        initial_capital=1000,\n",
    "        profit_threshold=0.10,\n",
    "        loss_threshold=-0.05,\n",
    "    )\n",
    "    result = simulator.simulate()\n",
    "    print(\"---------\")\n",
    "    \n",
    "# After the loop, print the final result\n",
    "print(f\"Simulated buying stock in day: {dataset['Date'][random_index]}\")\n",
    "print(f\"Simulation completed after {i} days:\")\n",
    "print(result)"
   ],
   "id": "b71ed8340b78d434",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
